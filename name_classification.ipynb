{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bcfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_PATH = 'data/NameCountryPytorch/names'\n",
    "ASCII = string.ascii_letters\n",
    "ASCII_COUNT = len(ASCII)\n",
    "COUNTRIES = os.listdir(NAMES_PATH)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "random.seed(1711)\n",
    "\n",
    "def char_onehot(c):\n",
    "    if c not in ASCII:\n",
    "        return []\n",
    "    \n",
    "    result = [0.0] * ASCII_COUNT    \n",
    "    result[ASCII.index(c)] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_name_onehot(name):\n",
    "    name_onehot = []\n",
    "\n",
    "    for c in name:\n",
    "        c = char_onehot(c)\n",
    "        if c:\n",
    "            name_onehot.append(c)\n",
    "\n",
    "    return name_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f69892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From path to a whole Dataset\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, name_original, name_unicode, name_tensor, label):\n",
    "        self.name_original = name_original  # Each row is a original name\n",
    "        self.name_unicode = name_unicode    # Each row is a unicoded name\n",
    "        self.name_tensor = name_tensor      # Each row is a name tensor\n",
    "        self.label = label                  # Each row is a country label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return [self.name_original[i] for i in idx],\\\n",
    "                [self.name_unicode[i] for i in idx],\\\n",
    "                [self.name_tensor[i] for i in idx],\\\n",
    "                [self.label[i] for i in idx]\n",
    "        \n",
    "        return self.name_original[idx],\\\n",
    "                self.name_unicode[idx],\\\n",
    "                self.name_tensor[idx],\\\n",
    "                self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ed465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, hn_out = self.rnn(x)\n",
    "        compare = (rnn_out[:,-1,:] == hn_out[-1]).all().item()\n",
    "        if not compare:\n",
    "            print(False)\n",
    "        rnn_out = rnn_out[:,-1,:] # batch_size, last_word, output_dim\n",
    "        rnn_logits = self.fcc(rnn_out)\n",
    "        rnn_softmax = self.softmax(rnn_logits)\n",
    "        return rnn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGRU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:,-1,:] # batch_size, last_word, output_dim\n",
    "        gru_logits = self.fcc(gru_out)\n",
    "        gru_softmax = self.softmax(gru_logits)\n",
    "        return gru_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:,-1,:] # batch_size, last_word, output_dim\n",
    "        lstm_logits = self.fcc(lstm_out)\n",
    "        lstm_softmax = self.softmax(lstm_logits)\n",
    "        return lstm_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dict(path):\n",
    "    dataset_dict = {} # name_length to dataset\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + \"/\" + filename, encoding='utf-8') as f:\n",
    "            for name in f.readlines():\n",
    "                name = name.strip()\n",
    "                nameunicode = unidecode(name.replace(\" \", \"\"))\n",
    "                name_onehot = ascii_name_onehot(nameunicode)\n",
    "                if name_onehot:\n",
    "                    name_len = len(name_onehot)\n",
    "                    if name_len not in dataset_dict:\n",
    "                        dataset_dict[len(name_onehot)] = [[], [], [], []]\n",
    "                    \n",
    "                    dataset_dict[name_len][0].append(name)\n",
    "                    dataset_dict[name_len][1].append(nameunicode)\n",
    "                    dataset_dict[name_len][2].append(name_onehot)\n",
    "                    dataset_dict[name_len][3].append(COUNTRIES.index(filename))\n",
    "\n",
    "    result = {}\n",
    "    for length, ds in dataset_dict.items():\n",
    "        ds[2] = torch.tensor(ds[2], dtype=torch.float)\n",
    "        ds[3] = torch.tensor(ds[3], dtype=torch.long)\n",
    "        result[length] = NameDataset(ds[0], ds[1], ds[2], ds[3])\n",
    "    \n",
    "    return result\n",
    "# train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = get_dataset_dict(NAMES_PATH)\n",
    "dataloader_dict = {i: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for i, ds in dataset_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee662894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    epochs = 200\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _, dataloader in dataloader_dict.items():\n",
    "            dl_loss = 0\n",
    "            for dl in dataloader:\n",
    "                origin_name, unicode_name, onehot_name, country_label = dl            \n",
    "                x_out = model(onehot_name)\n",
    "                loss = F.cross_entropy(x_out, country_label, reduction='mean')\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                dl_loss += loss.item()\n",
    "            epoch_loss += dl_loss / len(dataloader)\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch={epoch}: Loss={epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, name):\n",
    "    model.eval()\n",
    "    name = unidecode(name)\n",
    "    name_onehot = ascii_name_onehot(name)\n",
    "    name_onehot = torch.tensor(name_onehot)\n",
    "    name_onehot = name_onehot.unsqueeze(0)  # A batch of one datapoint\n",
    "    output = model(name_onehot)\n",
    "    predicted_idx = output.argmax(dim=1)\n",
    "    return COUNTRIES[predicted_idx.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    for file_name in os.listdir(NAMES_PATH):\n",
    "        pred_countries = []\n",
    "        with open(NAMES_PATH + \"/\" + file_name, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                name = line.strip().replace(\" \", \"\")\n",
    "                predicted_country = make_prediction(model, name)\n",
    "                pred_countries.append(predicted_country)\n",
    "        freq_dict = dict(Counter(pred_countries))\n",
    "        acc = freq_dict.get(file_name, 0) / len(pred_countries) * 100\n",
    "        print(f\"{file_name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = NameRNN(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(rnn_model)\n",
    "eval(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13752e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = NameGRU(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(gru_model)\n",
    "eval(gru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = NameLSTM(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(lstm_model)\n",
    "eval(lstm_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
