{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bcfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_PATH = 'data/NameClassifyingPytorch/names'\n",
    "ASCII = string.ascii_letters\n",
    "ASCII_COUNT = len(ASCII)\n",
    "COUNTRIES = os.listdir(NAMES_PATH)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "random.seed(1711)\n",
    "\n",
    "def char_onehot(c):\n",
    "    if c not in ASCII:\n",
    "        return []\n",
    "    \n",
    "    result = [0.0] * ASCII_COUNT    \n",
    "    result[ASCII.index(c)] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9de805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_name_onehot(name):\n",
    "    name_onehot = []\n",
    "\n",
    "    for c in name:\n",
    "        c = char_onehot(c)\n",
    "        if c:\n",
    "            name_onehot.append(c)\n",
    "\n",
    "    return name_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f69892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From path to a whole Dataset\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, name_original, name_unicode, name_tensor, label):\n",
    "        self.name_original = name_original  # Each row is a original name\n",
    "        self.name_unicode = name_unicode    # Each row is a unicoded name\n",
    "        self.name_tensor = name_tensor      # Each row is a name tensor\n",
    "        self.label = label                  # Each row is a country label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return [self.name_original[i] for i in idx],\\\n",
    "                [self.name_unicode[i] for i in idx],\\\n",
    "                [self.name_tensor[i] for i in idx],\\\n",
    "                [self.label[i] for i in idx]\n",
    "        \n",
    "        return self.name_original[idx],\\\n",
    "                self.name_unicode[idx],\\\n",
    "                self.name_tensor[idx],\\\n",
    "                self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359ed465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        rnn_out = rnn_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        rnn_logits = self.fcc(rnn_out)\n",
    "        rnn_softmax = self.softmax(rnn_logits)\n",
    "        return rnn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac5dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGRU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        gru_logits = self.fcc(gru_out)\n",
    "        gru_softmax = self.softmax(gru_logits)\n",
    "        return gru_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a5536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        lstm_logits = self.fcc(lstm_out)\n",
    "        lstm_softmax = self.softmax(lstm_logits)\n",
    "        return lstm_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca7ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dict(path):\n",
    "    dataset_dict = {} # name_length to dataset\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + \"/\" + filename, encoding='utf-8') as f:\n",
    "            for name in f.readlines():\n",
    "                name = name.strip()\n",
    "                nameunicode = unidecode(name.replace(\" \", \"\"))\n",
    "                name_onehot = ascii_name_onehot(nameunicode)\n",
    "                if name_onehot:\n",
    "                    name_len = len(name_onehot)\n",
    "                    if name_len not in dataset_dict:\n",
    "                        dataset_dict[len(name_onehot)] = [[], [], [], []]\n",
    "                    \n",
    "                    dataset_dict[name_len][0].append(name)\n",
    "                    dataset_dict[name_len][1].append(nameunicode)\n",
    "                    dataset_dict[name_len][2].append(name_onehot)\n",
    "                    dataset_dict[name_len][3].append(COUNTRIES.index(filename))\n",
    "\n",
    "    result = {}\n",
    "    for length, ds in dataset_dict.items():\n",
    "        ds[2] = torch.tensor(ds[2], dtype=torch.float)\n",
    "        ds[3] = torch.tensor(ds[3], dtype=torch.long)\n",
    "        result[length] = NameDataset(ds[0], ds[1], ds[2], ds[3])\n",
    "    \n",
    "    return result\n",
    "# train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd3b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = get_dataset_dict(NAMES_PATH)\n",
    "dataloader_dict = {i: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for i, ds in dataset_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee662894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    epochs = 200\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _, dataloader in dataloader_dict.items():\n",
    "            dl_loss = 0\n",
    "            for dl in dataloader:\n",
    "                origin_name, unicode_name, onehot_name, country_label = dl            \n",
    "                x_out = model(onehot_name)\n",
    "                loss = F.cross_entropy(x_out, country_label, reduction='mean')\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                dl_loss += loss.item()\n",
    "            epoch_loss += dl_loss / len(dataloader)\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch={epoch}: Loss={epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2510feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, name):\n",
    "    model.eval()\n",
    "    name = unidecode(name)\n",
    "    name_onehot = ascii_name_onehot(name)\n",
    "    name_onehot = torch.tensor(name_onehot)\n",
    "    name_onehot = name_onehot.unsqueeze(0)  # A batch of one datapoint\n",
    "    output = model(name_onehot)\n",
    "    predicted_idx = output.argmax(dim=1)\n",
    "    return COUNTRIES[predicted_idx.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    for file_name in os.listdir(NAMES_PATH):\n",
    "        pred_countries = []\n",
    "        with open(NAMES_PATH + \"/\" + file_name, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                name = line.strip().replace(\" \", \"\")\n",
    "                predicted_country = make_prediction(model, name)\n",
    "                pred_countries.append(predicted_country)\n",
    "        freq_dict = dict(Counter(pred_countries))\n",
    "        acc = freq_dict.get(file_name, 0) / len(pred_countries) * 100\n",
    "        print(f\"{file_name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd1faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=33.568166097694856\n",
      "Epoch=50: Loss=2.1459467083174477\n",
      "Epoch=100: Loss=1.4859893803127218\n",
      "Epoch=150: Loss=2.1872763089031353\n",
      "Czech.txt: 96.72%\n",
      "German.txt: 91.99%\n",
      "Arabic.txt: 100.00%\n",
      "Japanese.txt: 99.29%\n",
      "Chinese.txt: 92.91%\n",
      "Vietnamese.txt: 84.93%\n",
      "Russian.txt: 99.56%\n",
      "French.txt: 81.23%\n",
      "Irish.txt: 80.60%\n",
      "English.txt: 96.54%\n",
      "Spanish.txt: 88.26%\n",
      "Greek.txt: 99.51%\n",
      "Italian.txt: 97.18%\n",
      "Portuguese.txt: 48.65%\n",
      "Scottish.txt: 21.00%\n",
      "Dutch.txt: 92.93%\n",
      "Korean.txt: 89.36%\n",
      "Polish.txt: 92.09%\n"
     ]
    }
   ],
   "source": [
    "rnn_model = NameRNN(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(rnn_model)\n",
    "eval(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13752e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=23.42127067853795\n",
      "Epoch=50: Loss=1.1994503026970795\n",
      "Epoch=100: Loss=0.9302142684005948\n",
      "Epoch=150: Loss=0.9222299609238289\n",
      "Czech.txt: 97.11%\n",
      "German.txt: 89.78%\n",
      "Arabic.txt: 100.00%\n",
      "Japanese.txt: 99.50%\n",
      "Chinese.txt: 89.18%\n",
      "Vietnamese.txt: 84.93%\n",
      "Russian.txt: 99.63%\n",
      "French.txt: 89.89%\n",
      "Irish.txt: 78.02%\n",
      "English.txt: 96.26%\n",
      "Spanish.txt: 84.56%\n",
      "Greek.txt: 100.00%\n",
      "Italian.txt: 96.47%\n",
      "Portuguese.txt: 68.92%\n",
      "Scottish.txt: 47.00%\n",
      "Dutch.txt: 95.29%\n",
      "Korean.txt: 79.79%\n",
      "Polish.txt: 91.37%\n"
     ]
    }
   ],
   "source": [
    "gru_model = NameGRU(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(gru_model)\n",
    "eval(gru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e0fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=35.43046084012918\n",
      "Epoch=50: Loss=1.4640627551203835\n",
      "Epoch=100: Loss=1.047924185571055\n",
      "Epoch=150: Loss=0.9119874319611514\n",
      "Czech.txt: 96.92%\n",
      "German.txt: 92.82%\n",
      "Arabic.txt: 100.00%\n",
      "Japanese.txt: 99.50%\n",
      "Chinese.txt: 90.30%\n",
      "Vietnamese.txt: 86.30%\n",
      "Russian.txt: 99.64%\n",
      "French.txt: 86.64%\n",
      "Irish.txt: 79.31%\n",
      "English.txt: 96.97%\n",
      "Spanish.txt: 82.89%\n",
      "Greek.txt: 100.00%\n",
      "Italian.txt: 97.04%\n",
      "Portuguese.txt: 74.32%\n",
      "Scottish.txt: 6.00%\n",
      "Dutch.txt: 91.92%\n",
      "Korean.txt: 79.79%\n",
      "Polish.txt: 92.81%\n"
     ]
    }
   ],
   "source": [
    "lstm_model = NameLSTM(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(lstm_model)\n",
    "eval(lstm_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
