{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54160787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.338954Z",
     "iopub.status.busy": "2025-07-04T03:55:54.338451Z",
     "iopub.status.idle": "2025-07-04T03:55:54.343448Z",
     "shell.execute_reply": "2025-07-04T03:55:54.342445Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.338932Z"
    },
    "id": "54160787",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import bleu_score\n",
    "from torcheval.metrics import BLEUScore\n",
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bdb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.347731Z",
     "iopub.status.busy": "2025-07-04T03:55:54.347506Z",
     "iopub.status.idle": "2025-07-04T03:55:54.362537Z",
     "shell.execute_reply": "2025-07-04T03:55:54.361877Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.347715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(f\"Run on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47098d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.363437Z",
     "iopub.status.busy": "2025-07-04T03:55:54.363181Z",
     "iopub.status.idle": "2025-07-04T03:55:54.374212Z",
     "shell.execute_reply": "2025-07-04T03:55:54.373511Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.363419Z"
    },
    "id": "47098d92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ML cloud services\n",
    "google_colab = False\n",
    "azure_ml = False\n",
    "kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9767267",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    !mkdir -p data/Multi30k_HuggingFace\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm\n",
    "elif kaggle:\n",
    "    !pip install torcheval\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm\n",
    "elif azure_ml:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda71b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:58.283826Z",
     "iopub.status.busy": "2025-07-04T03:55:58.283033Z",
     "iopub.status.idle": "2025-07-04T03:55:58.287355Z",
     "shell.execute_reply": "2025-07-04T03:55:58.286558Z",
     "shell.execute_reply.started": "2025-07-04T03:55:58.283800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"data/Multi30k_HuggingFace\"\n",
    "if azure_ml:\n",
    "    dataset_path = \"Users/khoi.hoangdai/\" + \"data/Multi30k_HuggingFace\"\n",
    "elif kaggle:\n",
    "    dataset_path = \"/kaggle/input/untitled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086a53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:59.197637Z",
     "iopub.status.busy": "2025-07-04T03:55:59.197321Z",
     "iopub.status.idle": "2025-07-04T03:55:59.595783Z",
     "shell.execute_reply": "2025-07-04T03:55:59.595220Z",
     "shell.execute_reply.started": "2025-07-04T03:55:59.197614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dataset_path)\n",
    "train_set, val_set, test_set = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef28dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:01.584615Z",
     "iopub.status.busy": "2025-07-04T03:56:01.583957Z",
     "iopub.status.idle": "2025-07-04T03:56:01.593909Z",
     "shell.execute_reply": "2025-07-04T03:56:01.593285Z",
     "shell.execute_reply.started": "2025-07-04T03:56:01.584590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "def setseed(seed):\n",
    "    \"\"\"Set all seeds and deterministic CuDNN behavior\"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch (CPU and all GPUs)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # CuDNN configurations (critical for reproducibility)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "setseed(1711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3472c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:02.127635Z",
     "iopub.status.busy": "2025-07-04T03:56:02.126998Z",
     "iopub.status.idle": "2025-07-04T03:56:03.960236Z",
     "shell.execute_reply": "2025-07-04T03:56:03.959384Z",
     "shell.execute_reply.started": "2025-07-04T03:56:02.127606Z"
    },
    "id": "f3472c90",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use tokenizer from spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aa5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:03.961591Z",
     "iopub.status.busy": "2025-07-04T03:56:03.961349Z",
     "iopub.status.idle": "2025-07-04T03:56:08.785287Z",
     "shell.execute_reply": "2025-07-04T03:56:08.784701Z",
     "shell.execute_reply.started": "2025-07-04T03:56:03.961573Z"
    },
    "id": "ff4aa5d7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the token frequency dict, ignore tokens with low frequency\n",
    "en_token_dict = Counter()\n",
    "de_token_dict = Counter()\n",
    "unk, pad, sos, eos = '<unk>', '<pad>', '<sos>', '<eos>'\n",
    "special_tokens = [unk, pad, sos, eos]\n",
    "min_freq = 2\n",
    "\n",
    "for example in train_set:\n",
    "    en_tokens = [token.text.lower() for token in en_nlp.tokenizer(example['en'])]\n",
    "    de_tokens = [token.text.lower() for token in de_nlp.tokenizer(example['de'])]\n",
    "    en_token_dict.update(en_tokens)\n",
    "    de_token_dict.update(de_tokens)\n",
    "\n",
    "# No need to keep track of the frequency\n",
    "en_token_dict = [k for (k, v) in en_token_dict.items() if v >= min_freq]\n",
    "en_token_dict = special_tokens + en_token_dict\n",
    "en_token_dict = {value: index for (index, value) in enumerate(en_token_dict)}\n",
    "en_idx_token_dict = {value: key for (key, value) in en_token_dict.items()}\n",
    "\n",
    "de_token_dict = [k for (k, v) in de_token_dict.items() if v >= min_freq]\n",
    "de_token_dict = special_tokens + de_token_dict\n",
    "de_token_dict = {value: index for (index, value) in enumerate(de_token_dict)}\n",
    "de_idx_token_dict = {value: key for (key, value) in de_token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24256f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:10.856784Z",
     "iopub.status.busy": "2025-07-04T03:56:10.856160Z",
     "iopub.status.idle": "2025-07-04T03:56:10.860377Z",
     "shell.execute_reply": "2025-07-04T03:56:10.859762Z",
     "shell.execute_reply.started": "2025-07-04T03:56:10.856761Z"
    },
    "id": "24256f0e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if special tokens share the same index\n",
    "for special in special_tokens:\n",
    "    if not en_token_dict[special] == de_token_dict[special]:\n",
    "        print(f\"Token {special} mismatch between EN and DE dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2e6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:11.230001Z",
     "iopub.status.busy": "2025-07-04T03:56:11.229268Z",
     "iopub.status.idle": "2025-07-04T03:56:11.236072Z",
     "shell.execute_reply": "2025-07-04T03:56:11.235217Z",
     "shell.execute_reply.started": "2025-07-04T03:56:11.229974Z"
    },
    "id": "d1c2e6dd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create token list and token IDs for each sentence in the dataset\n",
    "def tokenize_example(example, en_nlp, de_nlp, en_token_dict, de_token_dict, sos, eos):\n",
    "    en_tokens, de_tokens = [], []\n",
    "    en_ids, de_ids = [], []\n",
    "    for token in en_nlp.tokenizer(example['en']):\n",
    "        token = token.text.lower()\n",
    "        if token not in en_token_dict:\n",
    "            token = unk\n",
    "\n",
    "        en_tokens.append(token)\n",
    "        en_ids.append(en_token_dict[token])\n",
    "\n",
    "    en_tokens = [sos] + en_tokens + [eos]\n",
    "    en_ids = [en_token_dict[sos]] + en_ids + [en_token_dict[eos]]\n",
    "\n",
    "    for token in de_nlp.tokenizer(example['de']):\n",
    "        token = token.text.lower()\n",
    "        if token not in de_token_dict:\n",
    "            token = unk\n",
    "\n",
    "        de_tokens.append(token)\n",
    "        de_ids.append(de_token_dict[token])\n",
    "\n",
    "    de_tokens = [sos] + de_tokens + [eos]\n",
    "    de_ids = [de_token_dict[sos]] + de_ids + [de_token_dict[eos]]\n",
    "\n",
    "    example['en_tokens'] = en_tokens\n",
    "    example['en_ids'] = en_ids\n",
    "    example['de_tokens'] = de_tokens\n",
    "    example['de_ids'] = de_ids\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339579a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:13.947527Z",
     "iopub.status.busy": "2025-07-04T03:56:13.946908Z",
     "iopub.status.idle": "2025-07-04T03:56:22.368529Z",
     "shell.execute_reply": "2025-07-04T03:56:22.367714Z",
     "shell.execute_reply.started": "2025-07-04T03:56:13.947503Z"
    },
    "id": "9339579a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fn_kwargs = {\n",
    "    'en_nlp': en_nlp,\n",
    "    'de_nlp': de_nlp,\n",
    "    'en_token_dict': en_token_dict,\n",
    "    'de_token_dict': de_token_dict,\n",
    "    'sos': sos,\n",
    "    'eos': eos,\n",
    "}\n",
    "train_set = train_set.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "val_set = val_set.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_set = test_set.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63aed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:22.369913Z",
     "iopub.status.busy": "2025-07-04T03:56:22.369686Z",
     "iopub.status.idle": "2025-07-04T03:56:22.377684Z",
     "shell.execute_reply": "2025-07-04T03:56:22.376882Z",
     "shell.execute_reply.started": "2025-07-04T03:56:22.369896Z"
    },
    "id": "ea63aed1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_set[0]['de'])\n",
    "print(train_set[0]['de_tokens'])\n",
    "print(train_set[0]['de_ids'])\n",
    "print(train_set[0]['en'])\n",
    "print(train_set[0]['en_tokens'])\n",
    "print(train_set[0]['en_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36998e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:22.378689Z",
     "iopub.status.busy": "2025-07-04T03:56:22.378401Z",
     "iopub.status.idle": "2025-07-04T03:56:22.419741Z",
     "shell.execute_reply": "2025-07-04T03:56:22.418969Z",
     "shell.execute_reply.started": "2025-07-04T03:56:22.378665Z"
    },
    "id": "e36998e4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Write a collate_fn to pad sequences with variable length into a batch of tensors for Dataloader\n",
    "def get_collate_fn(pad_index=1):\n",
    "    def collate_fn(batch):\n",
    "        # Encoder input: <sequence> + <eos>\n",
    "        encoder_input = [torch.tensor(sequence['de_ids'][1:]) for sequence in batch]\n",
    "        encoder_input = rnn.pad_sequence(encoder_input, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        # Decode input: <sos> + <sequence>\n",
    "        decoder_input = [torch.tensor(sequence['en_ids'][:-1]) for sequence in batch]\n",
    "        decoder_input = rnn.pad_sequence(decoder_input, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        # Decode output: <sequence> + <eos>\n",
    "        decoder_output = [torch.tensor(sequence['en_ids'][1:]) for sequence in batch]\n",
    "        decoder_output = rnn.pad_sequence(decoder_output, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        return encoder_input, decoder_input, decoder_output\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06c009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:28.925118Z",
     "iopub.status.busy": "2025-07-04T03:56:28.924575Z",
     "iopub.status.idle": "2025-07-04T03:56:28.929442Z",
     "shell.execute_reply": "2025-07-04T03:56:28.928675Z",
     "shell.execute_reply.started": "2025-07-04T03:56:28.925093Z"
    },
    "id": "eb06c009",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collate_fn = get_collate_fn()\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27fe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:37.976480Z",
     "iopub.status.busy": "2025-07-04T03:56:37.976189Z",
     "iopub.status.idle": "2025-07-04T03:56:38.831309Z",
     "shell.execute_reply": "2025-07-04T03:56:38.830739Z",
     "shell.execute_reply.started": "2025-07-04T03:56:37.976458Z"
    },
    "id": "fc27fe3b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, epochs, loss_fn=F.cross_entropy, dataloader=train_dl, pad_idx=1):\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_start = datetime.now()\n",
    "        next_chunk = 0\n",
    "        for idx, dl in enumerate(dataloader):\n",
    "            batch_start = datetime.now()\n",
    "            encoder_input, decoder_input, decoder_output = dl\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_output = decoder_output.to(device)\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            loss = loss_fn(output.permute(0, 2, 1), decoder_output, ignore_index=pad_idx, reduction='mean')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_runtime = datetime.now() - batch_start\n",
    "            if idx == next_chunk:\n",
    "                print(f\"Chunk={idx}: loss={loss.item():.2f}, batch runtime={batch_runtime.total_seconds()*1000:.2f} ms\")\n",
    "                next_chunk += len(train_dl) // 10\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "        epoch_runtime = datetime.now() - epoch_start\n",
    "        print(f\"Epoch={epoch}: Loss={epoch_loss / len(train_dl):.2f}, epoch runtime={epoch_runtime.seconds:.2f} seconds\")\n",
    "\n",
    "    return total_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14678e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_token_dict)\n",
    "output_dim = len(en_token_dict)\n",
    "emb_dim = 512\n",
    "attn_dim = 64\n",
    "att_heads = 8\n",
    "ffn_dim = 2048\n",
    "layers = 6\n",
    "max_seq_len = 50\n",
    "epochs = 20\n",
    "\n",
    "# Transformer\n",
    "transformer_model = Transformer(input_dim, output_dim,\n",
    "        emb_dim=emb_dim, attn_dim=attn_dim, attn_heads=att_heads, \n",
    "        ffn_dim=ffn_dim, layers=layers, max_seq_len=max_seq_len\n",
    "    ).to(device)\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9571b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_DIR = \"saved_weights\"\n",
    "MODEL_FILE = f\"transfomer_epochs={epochs}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5974b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:59.497467Z",
     "iopub.status.busy": "2025-07-04T03:56:59.497181Z",
     "iopub.status.idle": "2025-07-04T04:09:41.219344Z",
     "shell.execute_reply": "2025-07-04T04:09:41.218420Z",
     "shell.execute_reply.started": "2025-07-04T03:56:59.497445Z"
    },
    "id": "c3a5974b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = transformer_model\n",
    "optimizer = transformer_optimizer\n",
    "train_gru_err = train_fn(model, optimizer, epochs)\n",
    "torch.save(model.state_dict(), MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a810",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_blue_tokenizer(en_nlp, en_token_dict, unk):\n",
    "    def blue_tokenizer(s):\n",
    "        en_tokens = [token.text.lower() if token.text.lower() in en_token_dict else unk for token in en_nlp.tokenizer(s)]\n",
    "        return en_tokens\n",
    "\n",
    "    return blue_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbe569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_idx_to_sentence(indices, en_idx_token_dict, pad_idx=1):\n",
    "    sentence = [en_idx_token_dict[idx.item()] for idx in indices if idx.item() != pad_idx]\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5922f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, model_file, val_set=val_set, collate_fn=collate_fn, batch_size=batch_size, en_idx_token_dict=en_idx_token_dict, eos=eos, device=device, max_output_len=30):\n",
    "    val_dl = DataLoader(val_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "    bleu_metric = BLEUScore(\n",
    "        n_gram=4\n",
    "    )\n",
    "    with open(f\"bleu_output/{model_file}_bleu.txt\", \"w\") as f:\n",
    "        with torch.no_grad():\n",
    "            for idx, dl in enumerate(val_dl):\n",
    "                encoder_input, _, decoder_output = dl\n",
    "                encoder_input = encoder_input.to(device)\n",
    "                decoder_output = decoder_output.to(device)\n",
    "\n",
    "                for i, seq_input in enumerate(encoder_input):\n",
    "                    translated = model.translate(seq_input, en_idx_token_dict=en_idx_token_dict, device=device, eos=eos, sos_idx=2, max_output_len=max_output_len)\n",
    "                    #  = model.translate(model, seq_input, en_idx_token_dict, device)\n",
    "                    en_groud_truth = en_idx_to_sentence(decoder_output[i], en_idx_token_dict)\n",
    "                    translated_sentence = \" \".join(translated)\n",
    "\n",
    "                    bleu_metric.update([en_groud_truth], [[translated_sentence]])\n",
    "                    \n",
    "                    try:\n",
    "                        blue_results = bleu_score([translated_sentence], [en_groud_truth])\n",
    "                        print(en_groud_truth)\n",
    "                        print(translated_sentence)\n",
    "                        print(blue_results)       \n",
    "                        \n",
    "                        f.write(en_groud_truth + \"\\n\")\n",
    "                        f.write(translated_sentence + \"\\n\")\n",
    "                        f.write(str(blue_results.item()) + \"\\n\")\n",
    "                        f.write(\"=======================\" +  \"\\n\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "            scopus_level_bleu = bleu_metric.compute().item()\n",
    "            print(\"Scopus level blue: \" + str(scopus_level_bleu))\n",
    "            f.write(f\"Scopus level blue: {scopus_level_bleu}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"saved_weights\"\n",
    "\n",
    "for bidirectional in [True, False]:\n",
    "    for attention_type in ['dot_product', 'scaled_dot_product', 'generalized_dot_product']:\n",
    "        MODEL_FILE = f\"seq2seq_gru_attn={attention_type}_bidirectional={bidirectional}_layer={rnn_num_layers}_epochs={epochs}.pt\"\n",
    "        # Load the Seq2Seq model by first initializing the architecture of Encoder and Decoder\n",
    "        reload_seq2seq = Seq2Seq_GRU_Attention(input_dim, input_emb_dim,\n",
    "                                                 output_dim, output_emb_dim,\n",
    "                                                 rnn_hidden_dim, rnn_num_layers, attention_type,\n",
    "                                                 bidirectional=bidirectional).to(device)\n",
    "        reload_seq2seq.load_state_dict(torch.load(MODEL_DIR + \"/\" + MODEL_FILE, weights_only=False, map_location=device))\n",
    "        evaluate(reload_seq2seq, MODEL_FILE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7769574,
     "sourceId": 12325962,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
