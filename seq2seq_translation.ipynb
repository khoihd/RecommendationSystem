{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54160787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.338954Z",
     "iopub.status.busy": "2025-07-04T03:55:54.338451Z",
     "iopub.status.idle": "2025-07-04T03:55:54.343448Z",
     "shell.execute_reply": "2025-07-04T03:55:54.342445Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.338932Z"
    },
    "id": "54160787",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bdb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.347731Z",
     "iopub.status.busy": "2025-07-04T03:55:54.347506Z",
     "iopub.status.idle": "2025-07-04T03:55:54.362537Z",
     "shell.execute_reply": "2025-07-04T03:55:54.361877Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.347715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(f\"Run on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47098d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:54.363437Z",
     "iopub.status.busy": "2025-07-04T03:55:54.363181Z",
     "iopub.status.idle": "2025-07-04T03:55:54.374212Z",
     "shell.execute_reply": "2025-07-04T03:55:54.373511Z",
     "shell.execute_reply.started": "2025-07-04T03:55:54.363419Z"
    },
    "id": "47098d92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ML cloud services\n",
    "google_colab = False\n",
    "azure_ml = False\n",
    "kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9767267",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    !mkdir -p data/Multi30k_HuggingFace\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm\n",
    "elif kaggle:\n",
    "    !pip install torcheval\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm\n",
    "elif azure_ml:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda71b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:58.283826Z",
     "iopub.status.busy": "2025-07-04T03:55:58.283033Z",
     "iopub.status.idle": "2025-07-04T03:55:58.287355Z",
     "shell.execute_reply": "2025-07-04T03:55:58.286558Z",
     "shell.execute_reply.started": "2025-07-04T03:55:58.283800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"data/Multi30k_HuggingFace\"\n",
    "if azure_ml:\n",
    "    dataset_path = \"Users/khoi.hoangdai/\" + \"data/Multi30k_HuggingFace\"\n",
    "elif kaggle:\n",
    "    dataset_path = \"/kaggle/input/untitled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086a53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:55:59.197637Z",
     "iopub.status.busy": "2025-07-04T03:55:59.197321Z",
     "iopub.status.idle": "2025-07-04T03:55:59.595783Z",
     "shell.execute_reply": "2025-07-04T03:55:59.595220Z",
     "shell.execute_reply.started": "2025-07-04T03:55:59.197614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dataset_path)\n",
    "train_set, val_set, test_set = dataset['train'], dataset['validation'], dataset['test']\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef28dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:01.584615Z",
     "iopub.status.busy": "2025-07-04T03:56:01.583957Z",
     "iopub.status.idle": "2025-07-04T03:56:01.593909Z",
     "shell.execute_reply": "2025-07-04T03:56:01.593285Z",
     "shell.execute_reply.started": "2025-07-04T03:56:01.584590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "def setseed(seed):\n",
    "    \"\"\"Set all seeds and deterministic CuDNN behavior\"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch (CPU and all GPUs)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # CuDNN configurations (critical for reproducibility)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "setseed(1711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3472c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:02.127635Z",
     "iopub.status.busy": "2025-07-04T03:56:02.126998Z",
     "iopub.status.idle": "2025-07-04T03:56:03.960236Z",
     "shell.execute_reply": "2025-07-04T03:56:03.959384Z",
     "shell.execute_reply.started": "2025-07-04T03:56:02.127606Z"
    },
    "id": "f3472c90",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use tokenizer from spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aa5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:03.961591Z",
     "iopub.status.busy": "2025-07-04T03:56:03.961349Z",
     "iopub.status.idle": "2025-07-04T03:56:08.785287Z",
     "shell.execute_reply": "2025-07-04T03:56:08.784701Z",
     "shell.execute_reply.started": "2025-07-04T03:56:03.961573Z"
    },
    "id": "ff4aa5d7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the token frequency dict, ignore tokens with low frequency\n",
    "en_token_dict = Counter()\n",
    "de_token_dict = Counter()\n",
    "unk, pad, sos, eos = '<unk>', '<pad>', '<sos>', '<eos>'\n",
    "special_tokens = [unk, pad, sos, eos]\n",
    "min_freq = 2\n",
    "\n",
    "for example in train_set:\n",
    "    en_tokens = [token.text.lower() for token in en_nlp.tokenizer(example['en'])]\n",
    "    de_tokens = [token.text.lower() for token in de_nlp.tokenizer(example['de'])]\n",
    "    en_token_dict.update(en_tokens)\n",
    "    de_token_dict.update(de_tokens)\n",
    "\n",
    "# No need to keep track of the frequency\n",
    "en_token_dict = [k for (k, v) in en_token_dict.items() if v >= min_freq]\n",
    "en_token_dict = special_tokens + en_token_dict\n",
    "en_token_dict = {value: index for (index, value) in enumerate(en_token_dict)}\n",
    "en_idx_token_dict = {value: key for (key, value) in en_token_dict.items()}\n",
    "\n",
    "de_token_dict = [k for (k, v) in de_token_dict.items() if v >= min_freq]\n",
    "de_token_dict = special_tokens + de_token_dict\n",
    "de_token_dict = {value: index for (index, value) in enumerate(de_token_dict)}\n",
    "de_idx_token_dict = {value: key for (key, value) in de_token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24256f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:10.856784Z",
     "iopub.status.busy": "2025-07-04T03:56:10.856160Z",
     "iopub.status.idle": "2025-07-04T03:56:10.860377Z",
     "shell.execute_reply": "2025-07-04T03:56:10.859762Z",
     "shell.execute_reply.started": "2025-07-04T03:56:10.856761Z"
    },
    "id": "24256f0e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if special tokens share the same index\n",
    "for special in special_tokens:\n",
    "    if not en_token_dict[special] == de_token_dict[special]:\n",
    "        print(f\"Token {special} mismatch between EN and DE dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2e6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:11.230001Z",
     "iopub.status.busy": "2025-07-04T03:56:11.229268Z",
     "iopub.status.idle": "2025-07-04T03:56:11.236072Z",
     "shell.execute_reply": "2025-07-04T03:56:11.235217Z",
     "shell.execute_reply.started": "2025-07-04T03:56:11.229974Z"
    },
    "id": "d1c2e6dd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create token list and token IDs for each sentence in the dataset\n",
    "def tokenize_example(example, en_nlp, de_nlp, en_token_dict, de_token_dict, sos, eos):\n",
    "    en_tokens, de_tokens = [], []\n",
    "    en_ids, de_ids = [], []\n",
    "    for token in en_nlp.tokenizer(example['en']):\n",
    "        token = token.text.lower()\n",
    "        if token not in en_token_dict:\n",
    "            token = unk\n",
    "\n",
    "        en_tokens.append(token)\n",
    "        en_ids.append(en_token_dict[token])\n",
    "\n",
    "    en_tokens = [sos] + en_tokens + [eos]\n",
    "    en_ids = [en_token_dict[sos]] + en_ids + [en_token_dict[eos]]\n",
    "\n",
    "    for token in de_nlp.tokenizer(example['de']):\n",
    "        token = token.text.lower()\n",
    "        if token not in de_token_dict:\n",
    "            token = unk\n",
    "\n",
    "        de_tokens.append(token)\n",
    "        de_ids.append(de_token_dict[token])\n",
    "\n",
    "    de_tokens = [sos] + de_tokens + [eos]\n",
    "    de_ids = [de_token_dict[sos]] + de_ids + [de_token_dict[eos]]\n",
    "\n",
    "    example['en_tokens'] = en_tokens\n",
    "    example['en_ids'] = en_ids\n",
    "    example['de_tokens'] = de_tokens\n",
    "    example['de_ids'] = de_ids\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339579a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:13.947527Z",
     "iopub.status.busy": "2025-07-04T03:56:13.946908Z",
     "iopub.status.idle": "2025-07-04T03:56:22.368529Z",
     "shell.execute_reply": "2025-07-04T03:56:22.367714Z",
     "shell.execute_reply.started": "2025-07-04T03:56:13.947503Z"
    },
    "id": "9339579a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fn_kwargs = {\n",
    "    'en_nlp': en_nlp,\n",
    "    'de_nlp': de_nlp,\n",
    "    'en_token_dict': en_token_dict,\n",
    "    'de_token_dict': de_token_dict,\n",
    "    'sos': sos,\n",
    "    'eos': eos,\n",
    "}\n",
    "train_set = train_set.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "val_set = val_set.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_set = test_set.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63aed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:22.369913Z",
     "iopub.status.busy": "2025-07-04T03:56:22.369686Z",
     "iopub.status.idle": "2025-07-04T03:56:22.377684Z",
     "shell.execute_reply": "2025-07-04T03:56:22.376882Z",
     "shell.execute_reply.started": "2025-07-04T03:56:22.369896Z"
    },
    "id": "ea63aed1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_set[0]['de'])\n",
    "print(train_set[0]['de_tokens'])\n",
    "print(train_set[0]['de_ids'])\n",
    "print(train_set[0]['en'])\n",
    "print(train_set[0]['en_tokens'])\n",
    "print(train_set[0]['en_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36998e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:22.378689Z",
     "iopub.status.busy": "2025-07-04T03:56:22.378401Z",
     "iopub.status.idle": "2025-07-04T03:56:22.419741Z",
     "shell.execute_reply": "2025-07-04T03:56:22.418969Z",
     "shell.execute_reply.started": "2025-07-04T03:56:22.378665Z"
    },
    "id": "e36998e4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Write a collate_fn to pad sequences with variable length into a batch of tensors for Dataloader\n",
    "def get_collate_fn(pad_index=1):\n",
    "    def collate_fn(batch):\n",
    "        # Encoder input: <sequence> + <eos>\n",
    "        encoder_input = [torch.tensor(sequence['de_ids'][1:]) for sequence in batch]\n",
    "        encoder_input = rnn.pad_sequence(encoder_input, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        # Decode input: <sos> + <sequence>\n",
    "        decoder_input = [torch.tensor(sequence['en_ids'][:-1]) for sequence in batch]\n",
    "        decoder_input = rnn.pad_sequence(decoder_input, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        # Decode output: <sequence> + <eos>\n",
    "        decoder_output = [torch.tensor(sequence['en_ids'][1:]) for sequence in batch]\n",
    "        decoder_output = rnn.pad_sequence(decoder_output, padding_value=pad_index, batch_first=True)\n",
    "\n",
    "        return encoder_input, decoder_input, decoder_output\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06c009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:28.925118Z",
     "iopub.status.busy": "2025-07-04T03:56:28.924575Z",
     "iopub.status.idle": "2025-07-04T03:56:28.929442Z",
     "shell.execute_reply": "2025-07-04T03:56:28.928675Z",
     "shell.execute_reply.started": "2025-07-04T03:56:28.925093Z"
    },
    "id": "eb06c009",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collate_fn = get_collate_fn()\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d127bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:29.228407Z",
     "iopub.status.busy": "2025-07-04T03:56:29.228145Z",
     "iopub.status.idle": "2025-07-04T03:56:29.233345Z",
     "shell.execute_reply": "2025-07-04T03:56:29.232544Z",
     "shell.execute_reply.started": "2025-07-04T03:56:29.228388Z"
    },
    "id": "6d127bf9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, rnn_hidden_dim, rnn_num_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        # 1 layer Embedding\n",
    "        # 2 layers GRU\n",
    "        # the latent space is the same as the hidden space of the last layer of the GRU\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.encoder = nn.GRU(embedding_dim, rnn_hidden_dim, rnn_num_layers, bidirectional=bidirectional, batch_first=True, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state at the last layer for every word in the sequence:\n",
    "        #       batch, sequence, hidden_dim\n",
    "        # final hidden state at every layer\n",
    "        #       layer, batch, hidden_dim\n",
    "        x = self.embedding(x)\n",
    "        _, state_layer = self.encoder(x)\n",
    "        return state_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776f62d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:32.469658Z",
     "iopub.status.busy": "2025-07-04T03:56:32.469299Z",
     "iopub.status.idle": "2025-07-04T03:56:32.474699Z",
     "shell.execute_reply": "2025-07-04T03:56:32.473949Z",
     "shell.execute_reply.started": "2025-07-04T03:56:32.469635Z"
    },
    "id": "b776f62d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, rnn_hidden_dim, rnn_num_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        if bidirectional:\n",
    "            rnn_num_layers = rnn_num_layers * 2\n",
    "            \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.decoder = nn.GRU(embedding_dim, rnn_hidden_dim, rnn_num_layers, batch_first=True, bias=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        x = self.embedding(x)\n",
    "        # hidden state at the last layer for every word in the sequence:\n",
    "        #       batch, sequence, hidden_dim\n",
    "        # final hidden state at every layer\n",
    "        #       layer, batch, hidden_dim\n",
    "        state_sequence, state_layer = self.decoder(x, latent)\n",
    "        return self.fc(state_sequence), state_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3a507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:32.856843Z",
     "iopub.status.busy": "2025-07-04T03:56:32.856273Z",
     "iopub.status.idle": "2025-07-04T03:56:32.860903Z",
     "shell.execute_reply": "2025-07-04T03:56:32.860164Z",
     "shell.execute_reply.started": "2025-07-04T03:56:32.856823Z"
    },
    "id": "baa3a507",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        z = self.encoder(encoder_input)\n",
    "        decoder_output, decoder_state_layer = self.decoder(decoder_input, z)\n",
    "        return decoder_output, decoder_state_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615e7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:35.325211Z",
     "iopub.status.busy": "2025-07-04T03:56:35.324421Z",
     "iopub.status.idle": "2025-07-04T03:56:35.331277Z",
     "shell.execute_reply": "2025-07-04T03:56:35.330392Z",
     "shell.execute_reply.started": "2025-07-04T03:56:35.325187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, input_dim, input_emb_dim, encoder_hidden_dim, \n",
    "                    output_dim, output_emb_dim, decoder_hidden_dim, layer=1, bidirectional=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder_emb = nn.Embedding(input_dim, input_emb_dim)\n",
    "        self.encoder = nn.GRU(input_emb_dim, encoder_hidden_dim, layer, bias=True, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:\n",
    "            layer = layer * 2\n",
    "        self.decoder_emb = nn.Embedding(output_dim, output_emb_dim)\n",
    "        self.decoder = nn.GRU(output_emb_dim, decoder_hidden_dim, layer, bias=True, batch_first=True, bidirectional=False)\n",
    "        self.fcc = nn.Linear(decoder_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_emb_input = self.encoder_emb(encoder_input)\n",
    "        _, state = self.encoder(encoder_emb_input)\n",
    "        decoder_emb_input = self.decoder_emb(decoder_input)\n",
    "        decoder_output, decoder_state = self.decoder(decoder_emb_input, state)\n",
    "        return self.fcc(decoder_output), decoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, input_emb_dim, encoder_hidden_dim,\n",
    "                 output_dim, output_emb_dim, decoder_hidden_dim, layer=1, bidirectional=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder_emb = nn.Embedding(input_dim, input_emb_dim)\n",
    "        self.encoder = nn.LSTM(input_emb_dim, encoder_hidden_dim, layer, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:\n",
    "            layer = layer * 2\n",
    "        self.decoder_emb = nn.Embedding(output_dim, output_emb_dim)\n",
    "        self.decoder = nn.LSTM(output_emb_dim, decoder_hidden_dim, layer, batch_first=True)\n",
    "        self.fcc = nn.Linear(decoder_hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_emb_input = self.encoder_emb(encoder_input)\n",
    "        _, (state, cell) = self.encoder(encoder_emb_input)\n",
    "        decoder_emb_input = self.decoder_emb(decoder_input)\n",
    "        output, (state, cell) = self.decoder(decoder_emb_input, (state, cell))\n",
    "        return self.fcc(output), state, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27fe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:37.976480Z",
     "iopub.status.busy": "2025-07-04T03:56:37.976189Z",
     "iopub.status.idle": "2025-07-04T03:56:38.831309Z",
     "shell.execute_reply": "2025-07-04T03:56:38.830739Z",
     "shell.execute_reply.started": "2025-07-04T03:56:37.976458Z"
    },
    "id": "fc27fe3b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, epochs, loss_fn=F.cross_entropy, dataloader=train_dl, pad_idx=1):\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_start = datetime.now()\n",
    "        next_chunk = 0\n",
    "        for idx, dl in enumerate(dataloader):\n",
    "            batch_start = datetime.now()\n",
    "            encoder_input, decoder_input, decoder_output = dl\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_output = decoder_output.to(device)\n",
    "            output = model(encoder_input, decoder_input)[0]\n",
    "            loss = loss_fn(output.permute(0, 2, 1), decoder_output, ignore_index=pad_idx, reduction='mean')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_runtime = datetime.now() - batch_start\n",
    "            if idx == next_chunk:\n",
    "                print(f\"Chunk={idx}: loss={loss.item():.2f}, batch runtime={batch_runtime.total_seconds()*1000:.2f} ms\")\n",
    "                next_chunk += len(train_dl) // 10\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "        epoch_runtime = datetime.now() - epoch_start\n",
    "        print(f\"Epoch={epoch}: Loss={epoch_loss / len(train_dl):.2f}, epoch runtime={epoch_runtime.seconds:.2f} seconds\")\n",
    "\n",
    "    return total_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14678e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_token_dict)\n",
    "output_dim = len(en_token_dict)\n",
    "input_emb_dim = 256\n",
    "output_emb_dim = 256\n",
    "rnn_hidden_dim = 512\n",
    "rnn_num_layers = 4\n",
    "bidirectional = True\n",
    "\n",
    "# Encoder_GRU - Decoder_GRU\n",
    "encoder = Encoder_GRU(input_dim, input_emb_dim, rnn_hidden_dim, rnn_num_layers, bidirectional)\n",
    "decoder = Decoder_GRU(output_dim, output_emb_dim, rnn_hidden_dim, rnn_num_layers, bidirectional)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder).to(device)\n",
    "seq2seq_optimizer = optim.Adam(seq2seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Seq2Seq_GRU\n",
    "seq2seq_gru_model = Seq2Seq_GRU(input_dim, input_emb_dim, rnn_hidden_dim, \n",
    "                            output_dim, output_emb_dim, rnn_hidden_dim, \n",
    "                            rnn_num_layers, bidirectional).to(device)\n",
    "seq2seq_gru_optimizer = optim.Adam(seq2seq_gru_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Seq2Seq_LSTM\n",
    "seq2seq_lstm = Seq2Seq_LSTM(input_dim, input_emb_dim, rnn_hidden_dim,\n",
    "                            output_dim, output_emb_dim, rnn_hidden_dim,\n",
    "                            rnn_num_layers, bidirectional).to(device)\n",
    "seq2seq_lstm_optimizer = optim.Adam(seq2seq_lstm.parameters(), lr=1e-3)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5974b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:56:59.497467Z",
     "iopub.status.busy": "2025-07-04T03:56:59.497181Z",
     "iopub.status.idle": "2025-07-04T04:09:41.219344Z",
     "shell.execute_reply": "2025-07-04T04:09:41.218420Z",
     "shell.execute_reply.started": "2025-07-04T03:56:59.497445Z"
    },
    "id": "c3a5974b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = seq2seq_lstm\n",
    "optimizer = seq2seq_lstm_optimizer\n",
    "train_gru_err = train_fn(model, optimizer, epochs)\n",
    "\n",
    "MODEL_PATH = 'seq2seq_lstm_bidirectional_4_layer_20_epochs_kaggle.pt'\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b03c5c",
   "metadata": {
    "id": "b7b03c5c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the Seq2Seq model by first initializing the architecture of Encoder and Decoder\n",
    "MODEL_PATH = 'seq2seq_lstm_bidirectional_4_layer_20_epochs_kaggle.pt'\n",
    "# reload_encoder = Encoder(input_dim, encoder_embedding_dim, rnn_hidden_dim, rnn_num_layers, bidirectional)\n",
    "# reload_decoder = Decoder(output_dim, decoder_embedding_dim, rnn_hidden_dim, rnn_num_layers, bidirectional)\n",
    "# reload_seq2seq = Seq2Seq(reload_encoder, reload_decoder).to(device)\n",
    "\n",
    "# reload_seq2seq = Seq2Seq_GRU(input_dim, input_emb_dim, rnn_hidden_dim, \n",
    "#                             output_dim, output_emb_dim, rnn_hidden_dim, \n",
    "#                             rnn_num_layers, bidirectional\n",
    "#                 ).to(device)\n",
    "# reload_seq2seq.load_state_dict(torch.load(MODEL_PATH, weights_only=False, map_location=device))\n",
    "\n",
    "reload_seq2seq = Seq2Seq_LSTM(input_dim, input_emb_dim, rnn_hidden_dim,\n",
    "                            output_dim, output_emb_dim, rnn_hidden_dim,\n",
    "                            rnn_num_layers, bidirectional).to(device)\n",
    "reload_seq2seq.load_state_dict(torch.load(MODEL_PATH, weights_only=False, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee7c76",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate(model: Seq2Seq_GRU, sequence_tokens, en_idx_token_dict, device, sos_idx=2, max_length=100):\n",
    "    model.eval()\n",
    "    sequence_output = []\n",
    "    sequence_tokens_batch = sequence_tokens.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_input_emb = model.encoder_emb(sequence_tokens_batch)\n",
    "        _, state = model.encoder(encoder_input_emb)\n",
    "        word_idx = sos_idx\n",
    "        for _ in range(max_length):\n",
    "            word_encoder = torch.tensor([[word_idx]]).to(device)\n",
    "            word_emb = model.decoder_emb(word_encoder)\n",
    "            word_state, state = model.decoder(word_emb, state)\n",
    "            word_output = model.fcc(word_state)\n",
    "            \n",
    "            word_idx = torch.argmax(word_output, dim=-1).item()\n",
    "            word_token = en_idx_token_dict[word_idx]\n",
    "            sequence_output.append(word_token)\n",
    "            if word_token == eos:\n",
    "                break\n",
    "            \n",
    "    return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a810",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_blue_tokenizer(en_nlp, en_token_dict, unk):\n",
    "    def blue_tokenizer(s):\n",
    "        en_tokens = [token.text.lower() if token.text.lower() in en_token_dict else unk for token in en_nlp.tokenizer(s)]\n",
    "        return en_tokens\n",
    "\n",
    "    return blue_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ba094",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def en_idx_to_sentence(indices, en_idx_token_dict, pad_idx=1):\n",
    "    sentence = [en_idx_token_dict[idx.item()] for idx in indices if idx.item() != pad_idx]\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5922f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_dl = DataLoader(val_set, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "max_length = 30\n",
    "# def eval(model, val_dl):\n",
    "model = reload_seq2seq\n",
    "model.eval()\n",
    "tokenizer_fn = get_blue_tokenizer(en_nlp, en_token_dict, unk)\n",
    "\n",
    "all_translated = []\n",
    "all_ground_truth = []\n",
    "with open(f\"{MODEL_PATH}_bleu.txt\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for idx, dl in enumerate(val_dl):\n",
    "            encoder_input, _, decoder_output = dl\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_output = decoder_output.to(device)\n",
    "\n",
    "            for i, seq_input in enumerate(encoder_input):\n",
    "                translated = translate(model, seq_input, en_idx_token_dict, device)\n",
    "                en_groud_truth = en_idx_to_sentence(decoder_output[i], en_idx_token_dict)\n",
    "                translated_sentence = \" \".join(translated)\n",
    "                all_translated.append(translated_sentence)\n",
    "                all_ground_truth.append(en_groud_truth)\n",
    "                \n",
    "                blue_results = bleu_score([\" \".join(translated)], [en_groud_truth])\n",
    "                print(translated_sentence)\n",
    "                print(en_groud_truth)\n",
    "                print(blue_results)\n",
    "                f.write(translated_sentence + \"\\n\")\n",
    "                f.write(en_groud_truth + \"\\n\")\n",
    "                f.write(str(blue_results.item()) + \"\\n\")\n",
    "                f.write(\"=======================\" +  \"\\n\")\n",
    "\n",
    "    final_bleu = bleu_score(all_translated, all_ground_truth)\n",
    "    f.write(str(blue_results.item()) + \"\\n\")\n",
    "    print(str(final_bleu.item()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7769574,
     "sourceId": 12325962,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
