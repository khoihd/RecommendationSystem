{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bcfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d7579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_PATH = 'data/NameClassifyingPytorch/names'\n",
    "ASCII = string.ascii_letters\n",
    "ASCII_COUNT = len(ASCII)\n",
    "COUNTRIES = os.listdir(NAMES_PATH)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def char_onehot(c):\n",
    "    if c not in ASCII:\n",
    "        return []\n",
    "    \n",
    "    result = [0.0] * ASCII_COUNT    \n",
    "    result[ASCII.index(c)] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9de805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_name_onehot(name):\n",
    "    name_onehot = []\n",
    "\n",
    "    for c in name:\n",
    "        c = char_onehot(c)\n",
    "        if c:\n",
    "            name_onehot.append(c)\n",
    "\n",
    "    return name_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f69892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From path to a whole Dataset\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, name_original, name_unicode, name_tensor, label):\n",
    "        self.name_original = name_original  # Each row is a original name\n",
    "        self.name_unicode = name_unicode    # Each row is a unicoded name\n",
    "        self.name_tensor = name_tensor      # Each row is a name tensor\n",
    "        self.label = label                  # Each row is a country label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return [self.name_original[i] for i in idx],\\\n",
    "                [self.name_unicode[i] for i in idx],\\\n",
    "                [self.name_tensor[i] for i in idx],\\\n",
    "                [self.label[i] for i in idx]\n",
    "        \n",
    "        return self.name_original[idx],\\\n",
    "                self.name_unicode[idx],\\\n",
    "                self.name_tensor[idx],\\\n",
    "                self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "359ed465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        rnn_out = rnn_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        rnn_logits = self.fcc(rnn_out)\n",
    "        rnn_softmax = self.softmax(rnn_logits)\n",
    "        return rnn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ca7ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dict(path):\n",
    "    dataset_dict = {} # name_length to dataset\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + \"/\" + filename, encoding='utf-8') as f:\n",
    "            for name in f.readlines():\n",
    "                name = name.strip()\n",
    "                nameunicode = unidecode(name.replace(\" \", \"\"))\n",
    "                name_onehot = ascii_name_onehot(nameunicode)\n",
    "                if name_onehot:\n",
    "                    name_len = len(name_onehot)\n",
    "                    if name_len not in dataset_dict:\n",
    "                        dataset_dict[len(name_onehot)] = [[], [], [], []]\n",
    "                    \n",
    "                    dataset_dict[name_len][0].append(name)\n",
    "                    dataset_dict[name_len][1].append(nameunicode)\n",
    "                    dataset_dict[name_len][2].append(name_onehot)\n",
    "                    dataset_dict[name_len][3].append(COUNTRIES.index(filename))\n",
    "\n",
    "    result = {}\n",
    "    for length, ds in dataset_dict.items():\n",
    "        ds[2] = torch.tensor(ds[2], dtype=torch.float)\n",
    "        ds[3] = torch.tensor(ds[3], dtype=torch.long)\n",
    "        result[length] = NameDataset(ds[0], ds[1], ds[2], ds[3])\n",
    "    \n",
    "    return result\n",
    "# train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd3b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = get_dataset_dict(NAMES_PATH)\n",
    "dataloader_dict = {i: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for i, ds in dataset_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee662894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=26.920059372061147\n",
      "Epoch=10: Loss=13.290016304908846\n",
      "Epoch=20: Loss=6.065706416333093\n",
      "Epoch=30: Loss=4.065374626176362\n",
      "Epoch=40: Loss=4.545919353867724\n",
      "Epoch=50: Loss=2.415995747396278\n",
      "Epoch=60: Loss=5.273899072025392\n",
      "Epoch=70: Loss=1.8179100722297281\n",
      "Epoch=80: Loss=1.6721103832068798\n",
      "Epoch=90: Loss=1.5358747446631724\n",
      "Epoch=100: Loss=2.326447373587122\n",
      "Epoch=110: Loss=1.4832171308222915\n",
      "Epoch=120: Loss=1.3813077446240822\n",
      "Epoch=130: Loss=1.2674762396364718\n",
      "Epoch=140: Loss=1.2410983292330868\n",
      "Epoch=150: Loss=1.5091312807385435\n",
      "Epoch=160: Loss=1.2892449915147932\n",
      "Epoch=170: Loss=1.3659690868831642\n",
      "Epoch=180: Loss=1.9065759705607468\n",
      "Epoch=190: Loss=2.2338998299310244\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model = NameRNN(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for _, dataloader in dataloader_dict.items():\n",
    "        dl_loss = 0\n",
    "        for dl in dataloader:\n",
    "            origin_name, unicode_name, onehot_name, country_label = dl            \n",
    "            x_out = model(onehot_name)\n",
    "            loss = F.cross_entropy(x_out, country_label, reduction='mean')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            dl_loss += loss.item()\n",
    "        epoch_loss += dl_loss / len(dataloader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch={epoch}: Loss={epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2510feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(name):\n",
    "    model.eval()\n",
    "    name = unidecode(name)\n",
    "    name_onehot = ascii_name_onehot(name)\n",
    "    name_onehot = torch.tensor(name_onehot)\n",
    "    name_onehot = name_onehot.unsqueeze(0)  # A batch of one datapoint\n",
    "    output = model(name_onehot)\n",
    "    predicted_idx = output.argmax(dim=1)\n",
    "    return COUNTRIES[predicted_idx.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "613f90db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czech.txt: 96.53%\n",
      "{'German.txt': 651, 'English.txt': 22, 'Irish.txt': 3, 'French.txt': 15, 'Vietnamese.txt': 1, 'Russian.txt': 5, 'Dutch.txt': 14, 'Czech.txt': 10, 'Arabic.txt': 1, 'Spanish.txt': 1, 'Korean.txt': 1}\n",
      "German.txt: 89.92%\n",
      "Arabic.txt: 97.65%\n",
      "Japanese.txt: 99.29%\n",
      "Chinese.txt: 88.43%\n",
      "Vietnamese.txt: 82.19%\n",
      "Russian.txt: 99.61%\n",
      "French.txt: 90.61%\n",
      "Irish.txt: 88.79%\n",
      "English.txt: 95.28%\n",
      "Spanish.txt: 79.87%\n",
      "Greek.txt: 95.57%\n",
      "Italian.txt: 98.03%\n",
      "Portuguese.txt: 77.03%\n",
      "Scottish.txt: 48.00%\n",
      "Dutch.txt: 91.58%\n",
      "Korean.txt: 89.36%\n",
      "Polish.txt: 93.53%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for file_name in os.listdir(NAMES_PATH):\n",
    "    pred_countries = []\n",
    "    with open(NAMES_PATH + \"/\" + file_name) as f:\n",
    "        for line in f.readlines():\n",
    "            name = line.strip().replace(\" \", \"\")\n",
    "            predicted_country = make_prediction(name)\n",
    "            pred_countries.append(predicted_country)\n",
    "    freq_dict = dict(Counter(pred_countries))\n",
    "    if file_name == \"German.txt\":\n",
    "        print(freq_dict)\n",
    "    acc = freq_dict.get(file_name, 0) / len(pred_countries) * 100\n",
    "    print(f\"{file_name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
