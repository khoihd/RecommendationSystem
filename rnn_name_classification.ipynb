{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bcfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d7579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_PATH = 'data/NameClassifyingPytorch/names'\n",
    "ASCII = string.ascii_letters\n",
    "ASCII_COUNT = len(ASCII)\n",
    "COUNTRIES = os.listdir(NAMES_PATH)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def char_onehot(c):\n",
    "    if c not in ASCII:\n",
    "        return []\n",
    "    \n",
    "    result = [0.0] * ASCII_COUNT    \n",
    "    result[ASCII.index(c)] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9de805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_name_onehot(name):\n",
    "    name_onehot = []\n",
    "\n",
    "    for c in name:\n",
    "        c = char_onehot(c)\n",
    "        if c:\n",
    "            name_onehot.append(c)\n",
    "\n",
    "    return name_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f69892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From path to a whole Dataset\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, name_original, name_unicode, name_tensor, label):\n",
    "        self.name_original = name_original  # Each row is a original name\n",
    "        self.name_unicode = name_unicode    # Each row is a unicoded name\n",
    "        self.name_tensor = name_tensor      # Each row is a name tensor\n",
    "        self.label = label                  # Each row is a country label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return [self.name_original[i] for i in idx],\\\n",
    "                [self.name_unicode[i] for i in idx],\\\n",
    "                [self.name_tensor[i] for i in idx],\\\n",
    "                [self.label[i] for i in idx]\n",
    "        \n",
    "        return self.name_original[idx],\\\n",
    "                self.name_unicode[idx],\\\n",
    "                self.name_tensor[idx],\\\n",
    "                self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359ed465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        rnn_out = rnn_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        rnn_logits = self.fcc(rnn_out)\n",
    "        rnn_softmax = self.softmax(rnn_logits)\n",
    "        return rnn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac5dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGRU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bias=True, batch_first=True)\n",
    "        self.fcc = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:,-1,:] # batch_size, layer, dimensions\n",
    "        gru_logits = self.fcc(gru_out)\n",
    "        gru_softmax = self.softmax(gru_logits)\n",
    "        return gru_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca7ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dict(path):\n",
    "    dataset_dict = {} # name_length to dataset\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        with open(path + \"/\" + filename, encoding='utf-8') as f:\n",
    "            for name in f.readlines():\n",
    "                name = name.strip()\n",
    "                nameunicode = unidecode(name.replace(\" \", \"\"))\n",
    "                name_onehot = ascii_name_onehot(nameunicode)\n",
    "                if name_onehot:\n",
    "                    name_len = len(name_onehot)\n",
    "                    if name_len not in dataset_dict:\n",
    "                        dataset_dict[len(name_onehot)] = [[], [], [], []]\n",
    "                    \n",
    "                    dataset_dict[name_len][0].append(name)\n",
    "                    dataset_dict[name_len][1].append(nameunicode)\n",
    "                    dataset_dict[name_len][2].append(name_onehot)\n",
    "                    dataset_dict[name_len][3].append(COUNTRIES.index(filename))\n",
    "\n",
    "    result = {}\n",
    "    for length, ds in dataset_dict.items():\n",
    "        ds[2] = torch.tensor(ds[2], dtype=torch.float)\n",
    "        ds[3] = torch.tensor(ds[3], dtype=torch.long)\n",
    "        result[length] = NameDataset(ds[0], ds[1], ds[2], ds[3])\n",
    "    \n",
    "    return result\n",
    "# train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd3b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = get_dataset_dict(NAMES_PATH)\n",
    "dataloader_dict = {i: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for i, ds in dataset_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee662894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    epochs = 200\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _, dataloader in dataloader_dict.items():\n",
    "            dl_loss = 0\n",
    "            for dl in dataloader:\n",
    "                origin_name, unicode_name, onehot_name, country_label = dl            \n",
    "                x_out = model(onehot_name)\n",
    "                loss = F.cross_entropy(x_out, country_label, reduction='mean')\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                dl_loss += loss.item()\n",
    "            epoch_loss += dl_loss / len(dataloader)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch={epoch}: Loss={epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2510feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, name):\n",
    "    model.eval()\n",
    "    name = unidecode(name)\n",
    "    name_onehot = ascii_name_onehot(name)\n",
    "    name_onehot = torch.tensor(name_onehot)\n",
    "    name_onehot = name_onehot.unsqueeze(0)  # A batch of one datapoint\n",
    "    output = model(name_onehot)\n",
    "    predicted_idx = output.argmax(dim=1)\n",
    "    return COUNTRIES[predicted_idx.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    for file_name in os.listdir(NAMES_PATH):\n",
    "        pred_countries = []\n",
    "        with open(NAMES_PATH + \"/\" + file_name) as f:\n",
    "            for line in f.readlines():\n",
    "                name = line.strip().replace(\" \", \"\")\n",
    "                predicted_country = make_prediction(model, name)\n",
    "                pred_countries.append(predicted_country)\n",
    "        freq_dict = dict(Counter(pred_countries))\n",
    "        if file_name == \"German.txt\":\n",
    "            print(freq_dict)\n",
    "        acc = freq_dict.get(file_name, 0) / len(pred_countries) * 100\n",
    "        print(f\"{file_name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd1faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=29.466061256787633\n",
      "Epoch=10: Loss=12.59384837373893\n",
      "Epoch=20: Loss=7.039466823320813\n",
      "Epoch=30: Loss=4.759995821461458\n",
      "Epoch=40: Loss=2.7148205555454643\n",
      "Epoch=50: Loss=2.2154061262300995\n",
      "Epoch=60: Loss=1.8845877567073925\n",
      "Epoch=70: Loss=1.8495852750742126\n",
      "Epoch=80: Loss=1.4358441896432825\n",
      "Epoch=90: Loss=1.4376517851140058\n",
      "Epoch=100: Loss=1.483886699791049\n",
      "Epoch=110: Loss=1.8674178134771795\n",
      "Epoch=120: Loss=1.187523109188684\n",
      "Epoch=130: Loss=6.488517623916673\n",
      "Epoch=140: Loss=1.3353331479924058\n",
      "Epoch=150: Loss=1.6194514183626527\n",
      "Epoch=160: Loss=1.2919664841359806\n",
      "Epoch=170: Loss=1.6317933917027188\n",
      "Epoch=180: Loss=1.9819259822478283\n",
      "Epoch=190: Loss=2.0955305625179914\n",
      "Czech.txt: 76.30%\n",
      "{'German.txt': 611, 'Russian.txt': 30, 'Dutch.txt': 6, 'English.txt': 57, 'French.txt': 8, 'Czech.txt': 8, 'Italian.txt': 1, 'Arabic.txt': 1, 'Greek.txt': 1, 'Portuguese.txt': 1}\n",
      "German.txt: 84.39%\n",
      "Arabic.txt: 86.80%\n",
      "Japanese.txt: 93.54%\n",
      "Chinese.txt: 85.45%\n",
      "Vietnamese.txt: 82.19%\n",
      "Russian.txt: 97.72%\n",
      "French.txt: 75.09%\n",
      "Irish.txt: 75.00%\n",
      "English.txt: 90.10%\n",
      "Spanish.txt: 75.84%\n",
      "Greek.txt: 95.07%\n",
      "Italian.txt: 91.82%\n",
      "Portuguese.txt: 51.35%\n",
      "Scottish.txt: 31.00%\n",
      "Dutch.txt: 75.08%\n",
      "Korean.txt: 80.85%\n",
      "Polish.txt: 73.38%\n"
     ]
    }
   ],
   "source": [
    "model = NameRNN(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(model)\n",
    "eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13752e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0: Loss=26.738926375940018\n",
      "Epoch=10: Loss=6.128811904692457\n",
      "Epoch=20: Loss=3.1435142928273456\n",
      "Epoch=30: Loss=2.0501439740400214\n",
      "Epoch=40: Loss=1.5239607063984502\n",
      "Epoch=50: Loss=1.2511156156344683\n",
      "Epoch=60: Loss=1.139568912266375\n",
      "Epoch=70: Loss=1.0275797698468845\n",
      "Epoch=80: Loss=1.0744358611053677\n",
      "Epoch=90: Loss=1.1332311138819713\n",
      "Epoch=100: Loss=0.9249245838845859\n",
      "Epoch=110: Loss=0.9096556757481913\n",
      "Epoch=120: Loss=0.9099932961688206\n",
      "Epoch=130: Loss=0.8713164116664696\n",
      "Epoch=140: Loss=0.9617373589619271\n",
      "Epoch=150: Loss=0.8914289252196089\n",
      "Epoch=160: Loss=0.8614266479861493\n",
      "Epoch=170: Loss=0.8676715695721071\n",
      "Epoch=180: Loss=0.8046213090564028\n",
      "Epoch=190: Loss=0.871670348164656\n",
      "Czech.txt: 95.95%\n",
      "{'German.txt': 670, 'French.txt': 4, 'Russian.txt': 4, 'Vietnamese.txt': 1, 'English.txt': 21, 'Polish.txt': 1, 'Korean.txt': 1, 'Arabic.txt': 1, 'Dutch.txt': 15, 'Chinese.txt': 2, 'Spanish.txt': 1, 'Czech.txt': 3}\n",
      "German.txt: 92.54%\n",
      "Arabic.txt: 100.00%\n",
      "Japanese.txt: 99.50%\n",
      "Chinese.txt: 94.40%\n",
      "Vietnamese.txt: 83.56%\n",
      "Russian.txt: 99.70%\n",
      "French.txt: 84.12%\n",
      "Irish.txt: 87.50%\n",
      "English.txt: 95.50%\n",
      "Spanish.txt: 83.89%\n",
      "Greek.txt: 100.00%\n",
      "Italian.txt: 97.88%\n",
      "Portuguese.txt: 68.92%\n",
      "Scottish.txt: 39.00%\n",
      "Dutch.txt: 92.59%\n",
      "Korean.txt: 73.40%\n",
      "Polish.txt: 92.81%\n"
     ]
    }
   ],
   "source": [
    "model = NameGRU(input_dim=ASCII_COUNT, output_dim=len(COUNTRIES))\n",
    "train(model)\n",
    "eval(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoihd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
